---
title: "Human Activity Recognition Prediction"
author: "Sean Dunagan"
date: "November 20, 2015"
output: html_document
---
## Executive Summary
In this project, we aimed to build a prediction model that would generate predictions for data regarding the Human Activity Recognition dataset. I used Principal Component Analysis to increase the amount of information that could be gathered from the relationships between the covariates in the dataset. I used a Multinomial Regression Classification method to build the prediction model. Cross validation was used to train and test the model and generate bounds for an out-of-set error rate.

## Data Pre-Processing
For my algorithm I decided to focus on the data rows which contained non-NA values for at least 70% of the values in the row. I decided to denote values of 'NA', '', and '#DIV/0!' as NA values. After that I removed any columns which contained any NA values. I then ran nearZeroVar() to remove columns which contained virtually no variance as these columns would not provide much information for our model. The next step was to remove columns which would hinder the model's accuracy. It appeared to me that the num_window value represented a unique indentifier for a set of data set based on the user and the time. As such, I kept this row but removed the user_name and timestamp rows as those would just create noise in the model. 

## Principal Component Analysis
I used the preProcess(trainingSet[,-classe_index], method="pca", pcaComp=115) method (leaving out the classe variable since that is the result we are trying to predict) to produce principal component analysis on the dataset. Performing PCA allows us to potentially reduce the amount of covariates in our model. I also allows us to potentially gain more information from the same amount of covariates by exploring the relationship between the covariates.  The more PCA components we use in the model, the more accurate the model will be, but constructing the model will take longer. 

## Cross Validation
The concept behind cross validation is that we want to be able to test our prediction model on sample data before using it on the actual test set. As such, I took a portion of the training data set and set it aside to use it to test the model. This was done with createDataPartition(y=convertedArmBandData$classe, p=0.8, list=FALSE); it randomly assigned 80% of the data points to be used as training set points and 20% to be used as a test set.
I performed ten iterations of this process, generating a new random seed generator each time, and creating a new partition of training/testing set each time. I recorded the accuracy of the classifier during each iteration and took the average accuracy after the 10 iterations.

## Out of Set Error
If we delcare the accuracy after the 10 iterations to be the in-set-accuracy, then 1-in-set-accuracy would be the in-set error rate. This is the error rate regarding the set of data that we used to train the classifier. The error rate we experience while predicting on the testing set is the out-of-set error rate, since this data is outside of the set of data we used to train the model. Given that this set of data is separate from the one that we trained our model with, we expect the out of sample error rate to be ABOVE the in-set error rate.

The github repo directory containing my file for the project can be found at the link below. The full_data_rows.R file was the one used to build/train the model, not project.R

https://github.com/dunagan5887/datasciencecoursera/tree/ml_cp_all_data/machine_learning/course_project
